{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
    "\n",
    "#### Name: Aidan Fischer\n",
    "#### Stevens ID: 10447681"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Multi-Layer Perceptron (MLP) (50 Points)\n",
    "\n",
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the data loading, preprocessing, tokenization, and TF-IDF feature extraction.\n",
    "3. Implement MLP model, evaluation metrics, and Mini-batch GD with AdaGrad.\n",
    "4. Implement the MLP with Tensorflow and compare to your implementation.\n",
    "5. Analysis the results in the Conlusion part.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons, pandas) using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterlab in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (4.1.2)\n",
      "Requirement already satisfied: matplotlib in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (3.8.3)\n",
      "Requirement already satisfied: nltk in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.4.1.post1)\n",
      "Requirement already satisfied: tensorflow in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.15.0.post1)\n",
      "Requirement already satisfied: tensorflow_addons in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.23.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: ipykernel in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (6.29.2)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: jupyter-core in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (5.7.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.12.5)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.19.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.25.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (0.2.4)\n",
      "Requirement already satisfied: packaging in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (6.4)\n",
      "Requirement already satisfied: traitlets in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (5.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: click in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (4.66.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: ipykernel in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (6.29.2)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: jupyter-core in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (5.7.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.12.5)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.19.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.25.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (0.2.4)\n",
      "Requirement already satisfied: packaging in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (6.4)\n",
      "Requirement already satisfied: traitlets in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab->-r requirements.txt (line 1)) (5.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: click in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (4.66.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 6)) (2.15.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow_addons->-r requirements.txt (line 7)) (2.13.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 6)) (0.42.0)\n",
      "Requirement already satisfied: anyio in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: certifi in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jinja2>=3.0.3->jupyterlab->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: argon2-cffi in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.5.2)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (7.16.1)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (5.9.2)\n",
      "Requirement already satisfied: overrides in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: websocket-client in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 1)) (4.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (0.9.17)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (4.21.1)\n",
      "Requirement already satisfied: requests>=2.31 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (3.0.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipykernel->jupyterlab->-r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipykernel->jupyterlab->-r requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipykernel->jupyterlab->-r requirements.txt (line 1)) (8.22.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipykernel->jupyterlab->-r requirements.txt (line 1)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipykernel->jupyterlab->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipykernel->jupyterlab->-r requirements.txt (line 1)) (5.9.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: decorator in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (0.33.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: ptyprocess in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (21.2.0)\n",
      "Requirement already satisfied: webencodings in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: fqdn in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (2.4)\n",
      "Requirement already satisfied: uri-template in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.13)\n",
      "Requirement already satisfied: wcwidth in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (0.2.13)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 6)) (3.2.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (2.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: pycparser in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (2.8.19.20240106)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# you may not run this cell after the first installation\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing (5 points)\n",
    "\n",
    "* Download the dataset from Canvas\n",
    "* Load data to text and labels\n",
    "* Preprocessing\n",
    "* Tokenization\n",
    "* Split data\n",
    "* Feature extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to a2-data/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk_path = os.path.join('a2-data', 'nltk')\n",
    "nltk.download('stopwords', download_dir=nltk_path)\n",
    "nltk.data.path.append(nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    sys.stdout.write(str_ + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data\n",
    "\n",
    "- Load sentences and labels\n",
    "- Transform string labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_label(data_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\" Load sentences and labels from the specified path\n",
    "    Args:\n",
    "        data_path: data_path: path to the data file, e.g., 'a1-data/SMSSpamCollection'\n",
    "        sentences: the raw text list of all sentences\n",
    "    Returns:\n",
    "        labels: the label list of all sentences\n",
    "    \"\"\"\n",
    "    sentences, labels = [], []\n",
    "    # Start your code here (load text and label from files)\n",
    "    # Open the data file, loop through each line, and split into label and sentence.\n",
    "    # Append the label and sentence pair to their appropriate lists.\n",
    "    with open(data_path, mode=\"r\") as data_file:\n",
    "        for line in data_file.readlines():\n",
    "            label, sentence = line.split(\"\\t\")\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    # End\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'Arthur Conan Doyle': 0, 'Fyodor Dostoyevsky': 1, 'Jane Austen': 2}\n",
      "Number of sentences and labels: 19536 19536\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('a2-data', 'books.txt')\n",
    "sentences, labels = load_sentence_label(data_path)\n",
    "\n",
    "label_map = {}\n",
    "for label in sorted(list(set(labels))):\n",
    "    label_map[label] = len(label_map)\n",
    "labels = np.array([label_map[label] for label in labels], dtype=int)\n",
    "sentences = np.array(sentences, dtype=object)\n",
    "\n",
    "print('Label map:', label_map)\n",
    "print('Number of sentences and labels:', len(sentences), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(sentences: np.ndarray,\n",
    "                     labels: np.ndarray,\n",
    "                     test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Split the sentences and labels into training and test data by shuffling\n",
    "    Args:\n",
    "        sentences: A numpy array containing all sentences\n",
    "        labels: A number array containing label ids\n",
    "        test_ratio: A float number to calculate the number of test data\n",
    "\n",
    "    Returns:\n",
    "        train_sentences: A numpy array containing all training sentences\n",
    "        train_labels: A number array containing all training label ids\n",
    "        test_sentences: A numpy array containing all test sentences\n",
    "        test_labels: A number array containing all test label ids\n",
    "    \"\"\"\n",
    "    assert 0 < test_ratio < 1\n",
    "    assert len(sentences) == len(labels)\n",
    "\n",
    "    train_index, test_index = [], []\n",
    "    # Start your code here (split the index for training and test)\n",
    "    # Create a numpy random number generator, get the number of entries\n",
    "    # we need for the test set, then randomize a list of the integers from\n",
    "    # 0 to the length of the data. Take a subset of length \"test length\"\n",
    "    # from the shuffled integers, then assign the rest to the train indices\n",
    "    r = np.random.default_rng(seed=14515)\n",
    "    length = len(sentences)\n",
    "    test_n = int(length * test_ratio)\n",
    "    indices = np.array(range(length))\n",
    "    r.shuffle(indices)\n",
    "    train_index = indices[test_n:]\n",
    "    test_index = indices[:test_n]\n",
    "    # End\n",
    "\n",
    "    train_sentences, train_labels = sentences[train_index], labels[train_index]\n",
    "    test_sentences, test_labels = sentences[test_index], labels[test_index]\n",
    "    return train_sentences, train_labels, test_sentences, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 14067\n",
      "Validation data length: 1562\n",
      "Test data length: 3907\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "(train_sentences, train_labels,\n",
    "    test_sentences, test_labels) = train_test_split(sentences, labels, test_ratio)\n",
    "(train_sentences, train_labels,\n",
    "    valid_sentences, valid_labels) = train_test_split(train_sentences, train_labels, valid_ratio)\n",
    "\n",
    "print('Training data length:', len(train_sentences))\n",
    "print('Validation data length:', len(valid_sentences))\n",
    "print('Test data length:', len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_label(labels: np.ndarray, label_map: dict[str, int]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: The labels of a dataset \n",
    "        label_map: The mapping from label to label id\n",
    "    Returns:\n",
    "        label_count: The mapping from label to its count\n",
    "    \"\"\"\n",
    "    label_count = {key: 0 for key in label_map.keys()}\n",
    "    # Start your code here (count the number of each label)\n",
    "    # Create an inverted map from id to label, then for each label,\n",
    "    # update the count.\n",
    "    id_map = {value: key for key, value in label_map.items()}\n",
    "    for label in labels:\n",
    "        label_count[id_map[label]] += 1\n",
    "    # End\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: {'Arthur Conan Doyle': 1869, 'Fyodor Dostoyevsky': 4246, 'Jane Austen': 7952}\n",
      "Validation: {'Arthur Conan Doyle': 201, 'Fyodor Dostoyevsky': 464, 'Jane Austen': 897}\n",
      "Test: {'Arthur Conan Doyle': 468, 'Fyodor Dostoyevsky': 1234, 'Jane Austen': 2205}\n"
     ]
    }
   ],
   "source": [
    "print('Training:', count_label(train_labels, label_map))\n",
    "print('Validation:', count_label(valid_labels, label_map))\n",
    "print('Test:', count_label(test_labels, label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset statistics\n",
    "Fill this table with the statistics you just printed (double click this cell to edit)\n",
    "\n",
    "|                | Arthur Conan Doyle | Fyodor Dostoyevsky | Jane Austen | Total |\n",
    "|:--------------:|--------------------|--------------------|-------------|-------|\n",
    "|  **Training**  |                1869|                4246|         7952|  14067|\n",
    "| **Validation** |                 201|                 464|          897|   1562|\n",
    "|    **Test**    |                 468|                1234|         2205|   3907|\n",
    "|    **Total**   |                2538|                5944|        11054|  19536|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess\n",
    "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "\n",
    "    def apply(self, sentence: str) -> str:\n",
    "        \"\"\" Apply the preprocessing rules to the sentence\n",
    "        Args:\n",
    "            sentence: raw sentence\n",
    "        Returns:\n",
    "            sentence: clean sentence\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        if self.url:\n",
    "            sentence = Preprocessor.remove_url(sentence)\n",
    "        if self.punctuation:\n",
    "            sentence = Preprocessor.remove_punctuation(sentence)\n",
    "        if self.number:\n",
    "            sentence = Preprocessor.remove_number(sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(sentence: str) -> str:\n",
    "        \"\"\" Remove punctuations in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible punctuations\n",
    "        Returns:\n",
    "            sentence: sentence without punctuations\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = re.sub(r'[\\.\\,\\:\\;\\!\\?\\(\\)]', '', sentence)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(sentence: str) -> str:\n",
    "        \"\"\" Remove urls in text with re\n",
    "        Args:\n",
    "            sentence: sentence with possible urls\n",
    "        Returns:\n",
    "            sentence: sentence without urls\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Replace urls with a space. The regex here is a complicated regex expression\n",
    "        # that I built in the past that is generalized and safe (urls of any form, and\n",
    "        # tries to avoid non-urls that might look like a url)\n",
    "        \n",
    "        # See end of this notebook for a breakdown of this regex expression\n",
    "        sentence = re.sub(r'(?:https?://)?\\w+\\.\\w+(\\.\\w+?)?(?(1)|(?=/))(?:/\\S*?)*(?:[\\s]|$|[^\\s\\w\\\\\\/\\&\\?\\%\\#])', \"\", sentence, flags=re.MULTILINE)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_number(sentence: str) -> str:\n",
    "        \"\"\" Remove numbers in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible numbers\n",
    "        Returns:\n",
    "            sentence: sentence without numbers\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = re.sub(r'[0-9]', '', sentence)\n",
    "        # End\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{clean_sentence}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"wouldn't\", \"she's\", 'both', 'each', 'into', 'hasn', 'had', \"haven't\", 'until', 'which']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "print(list(stopwords_set)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\" Tokenize a sentence into tokens (words)\n",
    "    Args:\n",
    "        sentence: clean sentence\n",
    "    Returns:\n",
    "        tokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    # Start your code here\n",
    "    #     Step 1. Split sentence into words\n",
    "    #     Step 2. Extract word stem using the defined stemmer (PorterStemmer) by calling stemmer.stem(word)\n",
    "    #     Step 3. Remove stop words using the defined stopwords_set\n",
    "    # Split\n",
    "    for word in sentence.split(\" \"):\n",
    "        # Append stemmed word to words if not part of stopwords\n",
    "        w = stemmer.stem(word)\n",
    "        if w not in stopwords_set:\n",
    "            words.append(w)\n",
    "    # End\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"['interest', 'rate', 'trim', 'south', 'african', 'central', 'bank', 'lack', 'warn', 'hit', 'rand', 'surpris', 'market']\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "tokens = tokenize(clean_sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{tokens}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Extraction\n",
    "\n",
    "TF-IDF:\n",
    "$$\\text{TF-IDF}(t, d) = \\frac{f_{t, d}}{\\sum_{t'}{f_{t', d}}} \\times \\log{\\frac{N}{n_t}}$$\n",
    "\n",
    "- $t$: A term\n",
    "- $d$: A document. Here, we regard a sentence as a document\n",
    "- $f_{t, d}$: Number of term $t$ in $d$\n",
    "- $N$: Number of document\n",
    "- $n_t$: Number of document containing $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TfIdfEncoder:\n",
    "    def __init__(self):\n",
    "        self.vocab = defaultdict(int)\n",
    "        self.token2index = {}\n",
    "        self.df = defaultdict(int)\n",
    "        self.num_doc = 0\n",
    "        self.processor = Preprocessor()\n",
    "\n",
    "    def fit(self, sentences: Union[List[str], np.ndarray]) -> int:\n",
    "        \"\"\" Using the given texts to store key information in TF-IDF calculation\n",
    "            In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary and store the frequency of tokens (self.vocab).\n",
    "                2. Construct the document frequency map to tokens (self.df).\n",
    "                3. Construct the token to index map based on the frequency.\n",
    "                   The token with a higher frequency has the smaller index\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            token_num\n",
    "        \"\"\"\n",
    "        self.num_doc = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == len(sentences) - 1:\n",
    "                print_line('Fitting TF-IDF encoder:', (i + 1), '/', len(sentences))\n",
    "            # Start your code here (step 1 & 2)\n",
    "            # Get set of unique tokens in the sentence\n",
    "            # Add to the vocab the number of times that token appears in the sentence\n",
    "            # Add one to the document frequency for that token\n",
    "            for token in set(sentence):\n",
    "                self.vocab[token] += sentence.count(token)\n",
    "                self.df[token] += 1\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        # Start your code here (Step 3)\n",
    "        # Split vocab into a list of (token, frequency) pairs, sort that list by the frequency\n",
    "        # enumerate it to get the indices, then build token2index using that information\n",
    "        self.token2index = {token: index for index, (token, _) in \n",
    "                            enumerate(sorted(list(self.vocab.items()), key=lambda kv: kv[1], reverse=True\n",
    "                            ))}\n",
    "        # End\n",
    "        token_num = len(self.token2index) \n",
    "        print('The number of distinct tokens:', token_num)\n",
    "        return token_num\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
    "        \"\"\" Encode the sentences into TF-IDF feature vector\n",
    "            Note: if a token in a sentence does not exist in the fit encoder, we just ignore it.\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            features: A (n x token_num) matrix, where n is the number of sentences\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        features = np.zeros((n, len(self.token2index)))\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Encoding with TF-IDF encoder:', (i + 1), '/', n)\n",
    "            # Start your code (calculate TF-IDF)\n",
    "            # For each unique token that exists in the sentence\n",
    "            # ignore the token if it isn't in the vocab, then get the proper\n",
    "            # index from token2index. Calculate the TF-IDF for the (token, doc) \n",
    "            # pair and put that into the features matrix\n",
    "            for token in set(sentence):\n",
    "                if token in self.vocab:\n",
    "                    index = self.token2index[token]\n",
    "                    features[i, index] = sentence.count(token) / len(sentence) * np.log(n / self.df[token])\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 100 / 100\n",
      "The number of distinct tokens: 72\n",
      "Encoding with TF-IDF encoder: 10 / 10\n",
      "[[-0.40295239 -0.17194011 -0.22823824 -0.05680315 -0.20059679 -0.02800887\n",
      "  -0.08597005 -0.11411912 -0.08481612 -0.19698803 -0.02827204 -0.08239592\n",
      "  -0.02814115  0.          0.         -0.08069108 -0.02760343 -0.02746531\n",
      "  -0.02689703 -0.05408308 -0.0266029  -0.02599302 -0.05548009 -0.02567655\n",
      "  -0.0221869   0.         -0.04306916 -0.01986544  0.         -0.02878231\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.01492403  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.00985572  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.40988426 -0.2040476  -0.13742093 -0.11895947 -0.1380322  -0.08212025\n",
      "  -0.120028   -0.13742093 -0.12433776 -0.11197525 -0.05920846 -0.08627845\n",
      "  -0.05304091 -0.08494724 -0.02904717 -0.03379731 -0.04624658 -0.02875948\n",
      "  -0.02253154 -0.0226526  -0.01114257 -0.02177426 -0.02904717 -0.03764101\n",
      "  -0.00929294 -0.00497934 -0.00901972 -0.0041603  -0.00888536 -0.00602771\n",
      "   0.         -0.00725809 -0.00335323  0.         -0.00250134  0.\n",
      "   0.         -0.0062509   0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.00206402  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.00133724\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.39985011 -0.2333718  -0.1366696  -0.09523881 -0.17846079 -0.06708712\n",
      "  -0.12354978 -0.0683348  -0.08126095 -0.1213271  -0.05417397 -0.0394711\n",
      "  -0.08088474 -0.06477015 -0.06644322 -0.05153921 -0.0396696  -0.02631407\n",
      "  -0.03865441 -0.02590806 -0.0127439  -0.02490349 -0.02657729 -0.04920057\n",
      "  -0.01062846  0.         -0.04126387  0.         -0.01354973 -0.01378793\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.00714924  0.          0.          0.          0.          0.\n",
      "   0.         -0.00384344  0.          0.          0.          0.\n",
      "   0.          0.         -0.00281439  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.38874813 -0.29773179 -0.02964133 -0.14754064 -0.08931954 -0.14550063\n",
      "  -0.23818543 -0.05928266 -0.11749419 -0.08771267 -0.14686773 -0.08560615\n",
      "  -0.02923756  0.         -0.02882082 -0.08383489 -0.02867889 -0.02853538\n",
      "   0.          0.         -0.02763937 -0.0810172  -0.02882082  0.\n",
      "   0.          0.          0.          0.          0.         -0.0299037\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.03101097  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.39180113 -0.210049   -0.14937057 -0.12490744 -0.1740406  -0.1642405\n",
      "  -0.1020238  -0.10157199 -0.10065438 -0.12376211 -0.0651293  -0.08052656\n",
      "  -0.07661464 -0.02831575 -0.0522849  -0.01689866 -0.06358905 -0.01725569\n",
      "  -0.05069597 -0.0226526  -0.02228515 -0.05987921 -0.01161887  0.\n",
      "   0.         -0.00995868  0.          0.          0.         -0.00602771\n",
      "  -0.00342496 -0.00725809  0.          0.         -0.00250134  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.00168025  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]]\n",
      "Fitting TF-IDF encoder: 100 / 100\n",
      "The number of distinct tokens: 72\n",
      "Encoding with TF-IDF encoder: 10 / 10\n",
      "[[-0.40295239 -0.17194011 -0.22823824 -0.05680315 -0.20059679 -0.02800887\n",
      "  -0.08597005 -0.11411912 -0.08481612 -0.19698803 -0.02827204 -0.08239592\n",
      "  -0.02814115  0.          0.         -0.08069108 -0.02760343 -0.02746531\n",
      "  -0.02689703 -0.05408308 -0.0266029  -0.02599302 -0.05548009 -0.02567655\n",
      "  -0.0221869   0.         -0.04306916 -0.01986544  0.         -0.02878231\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.01492403  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.00985572  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.40988426 -0.2040476  -0.13742093 -0.11895947 -0.1380322  -0.08212025\n",
      "  -0.120028   -0.13742093 -0.12433776 -0.11197525 -0.05920846 -0.08627845\n",
      "  -0.05304091 -0.08494724 -0.02904717 -0.03379731 -0.04624658 -0.02875948\n",
      "  -0.02253154 -0.0226526  -0.01114257 -0.02177426 -0.02904717 -0.03764101\n",
      "  -0.00929294 -0.00497934 -0.00901972 -0.0041603  -0.00888536 -0.00602771\n",
      "   0.         -0.00725809 -0.00335323  0.         -0.00250134  0.\n",
      "   0.         -0.0062509   0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.00206402  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.00133724\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.39985011 -0.2333718  -0.1366696  -0.09523881 -0.17846079 -0.06708712\n",
      "  -0.12354978 -0.0683348  -0.08126095 -0.1213271  -0.05417397 -0.0394711\n",
      "  -0.08088474 -0.06477015 -0.06644322 -0.05153921 -0.0396696  -0.02631407\n",
      "  -0.03865441 -0.02590806 -0.0127439  -0.02490349 -0.02657729 -0.04920057\n",
      "  -0.01062846  0.         -0.04126387  0.         -0.01354973 -0.01378793\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.00714924  0.          0.          0.          0.          0.\n",
      "   0.         -0.00384344  0.          0.          0.          0.\n",
      "   0.          0.         -0.00281439  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.38874813 -0.29773179 -0.02964133 -0.14754064 -0.08931954 -0.14550063\n",
      "  -0.23818543 -0.05928266 -0.11749419 -0.08771267 -0.14686773 -0.08560615\n",
      "  -0.02923756  0.         -0.02882082 -0.08383489 -0.02867889 -0.02853538\n",
      "   0.          0.         -0.02763937 -0.0810172  -0.02882082  0.\n",
      "   0.          0.          0.          0.          0.         -0.0299037\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.03101097  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.39180113 -0.210049   -0.14937057 -0.12490744 -0.1740406  -0.1642405\n",
      "  -0.1020238  -0.10157199 -0.10065438 -0.12376211 -0.0651293  -0.08052656\n",
      "  -0.07661464 -0.02831575 -0.0522849  -0.01689866 -0.06358905 -0.01725569\n",
      "  -0.05069597 -0.0226526  -0.02228515 -0.05987921 -0.01161887  0.\n",
      "   0.         -0.00995868  0.          0.          0.         -0.00602771\n",
      "  -0.00342496 -0.00725809  0.          0.         -0.00250134  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.00168025  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "encoder = TfIdfEncoder()\n",
    "encoder.fit(train_sentences[:100])\n",
    "features = encoder.encode(train_sentences[:10])\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode training, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 14067 / 14067\n",
      "The number of distinct tokens: 99\n",
      "Encoding with TF-IDF encoder: 14067 / 14067\n",
      "Encoding with TF-IDF encoder: 1562 / 1562\n",
      "Encoding with TF-IDF encoder: 3907 / 3907\n",
      "The size of training set: (14067, 99) (14067, 3)\n",
      "The size of validation set: (1562, 99) (1562, 3)\n",
      "The size of test set: (3907, 99) (3907, 3)\n"
     ]
    }
   ],
   "source": [
    "num_class = 3\n",
    "\n",
    "encoder = TfIdfEncoder()\n",
    "vocab_size = encoder.fit(train_sentences)\n",
    "\n",
    "x_train = encoder.encode(train_sentences)\n",
    "x_valid = encoder.encode(valid_sentences)\n",
    "x_test = encoder.encode(test_sentences)\n",
    "\n",
    "y_train = np.zeros((len(train_labels), num_class))\n",
    "y_valid = np.zeros((len(valid_labels), num_class))\n",
    "y_test = np.zeros((len(test_labels), num_class))\n",
    "y_train[np.arange(len(train_labels)), train_labels] = 1\n",
    "y_valid[np.arange(len(valid_labels)), valid_labels] = 1\n",
    "y_test[np.arange(len(test_labels)), test_labels] = 1\n",
    "\n",
    "print('The size of training set:', x_train.shape, y_train.shape)\n",
    "print('The size of validation set:', x_valid.shape, y_valid.shape)\n",
    "print('The size of test set:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP (20 Points)\n",
    "In this section, you are required to implement a two-layer MLP model (input -> hidden layer -> output layer) with $L_2$ regularization from scratch. \n",
    "\n",
    "The objective function of LR for multi-class classification:\n",
    "\n",
    "$$J = L(\\mathbf{x}, \\mathbf{y} \\mid \\mathbf{w}, \\mathbf{b}) = -\\frac{1}{n}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$$\n",
    "\n",
    "- $z_1 = w_1x$\n",
    "- $h_1 = activation(z_1)$\n",
    "- $z_2 = w_2 h_1$\n",
    "- $\\hat{y} = softmax(z_2)$\n",
    "\n",
    "- $n$: Number of samples\n",
    "- $d$: Dimension of $\\mathbf{w}$\n",
    "- Here, you can use `sigmoid` as the activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 MLP Model (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\" The softmax activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "        axis: The dimension of x that needs to run softmax, default -1, i.e., the last dimension\n",
    "    Returns:\n",
    "        output: Softmax value of the specified dimension in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    # Get the exponential of x, sum across axis, then divide by the sum\n",
    "    x = np.exp(x)\n",
    "    exp_sum = np.sum(x, axis=axis, keepdims=True)\n",
    "    x /= exp_sum\n",
    "    # End\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" The sigmoid activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "    Returns:\n",
    "        output: Sigmoid value of each entry in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    # sigmoid = 1/(1+e^(-x))\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    # End\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP:\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        # Initialize an rng object, then use Glorot-Bengio normalized weight initialization\n",
    "        r = np.random.default_rng(seed=6666)\n",
    "        self.w1 = r.uniform(low=-np.sqrt(6 / (feature_dim + hidden_dim)), \n",
    "                            high=np.sqrt(6 / (feature_dim + hidden_dim)),\n",
    "                            size=(feature_dim, hidden_dim))\n",
    "        self.b1 = r.uniform(low=-np.sqrt(6 / (1 + hidden_dim)),\n",
    "                            high=np.sqrt(6 / (1 + hidden_dim)),\n",
    "                            size=(1, hidden_dim))\n",
    "        self.w2 = r.uniform(low=-np.sqrt(6 / (hidden_dim + num_class)), \n",
    "                            high=np.sqrt(6 / (hidden_dim + num_class)),\n",
    "                            size=(hidden_dim, num_class))\n",
    "        self.b2 = r.uniform(low=-np.sqrt(6 / (1 + num_class)), \n",
    "                            high=np.sqrt(6 / (1 + num_class)),\n",
    "                            size=(1, num_class))\n",
    "\n",
    "        # End\n",
    "        self.lambda_ = lambda_\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, x: np.ndarray, return_hiddens: bool = False) -> np.ndarray:\n",
    "        \"\"\" Forward process of logistic regression\n",
    "            Calculate y_hat using x\n",
    "        Args:\n",
    "            x: Input data\n",
    "            return_hiddens: If true the function will return h1 for gradient calculation\n",
    "        Returns:\n",
    "            y_hat: Output\n",
    "            h1: Hidden output, used for gradient calculation. Returned if return_hiddens is set to True\n",
    "        \"\"\"\n",
    "        y_hat = 0\n",
    "        h1 = 0, 0\n",
    "        w1, b1, w2, b2 = self.w1, self.b1, self.w2, self.b2\n",
    "        # Start your code here (calculate y_hat of MLP using x)\n",
    "        z1 = x @ w1 + b1\n",
    "        h1 = sigmoid(z1)\n",
    "        z2 = h1 @ w2 + b2\n",
    "        y_hat = softmax(z2)\n",
    "        # End\n",
    "        if return_hiddens:\n",
    "            return y_hat, h1\n",
    "        else:\n",
    "            return y_hat\n",
    "\n",
    "    def backward(self,\n",
    "                 x: np.ndarray,\n",
    "                 y_hat: np.ndarray,\n",
    "                 y: np.ndarray,\n",
    "                 h1: np.array) -> Tuple[np.ndarray, Union[float, np.ndarray], np.ndarray, Union[float, np.ndarray]]:\n",
    "        \"\"\" Backward process of logistic regression\n",
    "            Calculate the gradient of w and b\n",
    "        Args:\n",
    "            x: Input data\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "            h1: Hidden output of the hidden layer\n",
    "        Returns:\n",
    "            dw1: Gradient of w1\n",
    "            db1: Gradient of b1\n",
    "            dw2: Gradient of w2\n",
    "            db2: Gradient of b2\n",
    "        \"\"\"\n",
    "        w1, w2 = self.w1, self.w2\n",
    "        dw1, db1, dw2, db2 = 0.0, 0.0, 0.0, 0.0\n",
    "        n = len(x)\n",
    "        # Start your code here (calculate the gradient of w and b)\n",
    "        # dw2 = J/w2 = J/y_hat * y_hat/w2 + 2 lambda w2 \n",
    "        # = J/y_hat * y_hat/z2 * z2/w2\n",
    "        # Derivative of j with respect to y_hat = -1/n * sum(y / yhat)\n",
    "        # because d/dyhat (log(yhat)) = 1/yhat\n",
    "        djyhat = -1 / n * y / y_hat\n",
    "        # Derivate of yhat with respect to z2 is the derivative of the softmax function\n",
    "        # = d/dz2 (e^z2/sum(e^z2)) = (e^z2sum(e^z2)-e^2z2))/(sum(e^z2)^2)\n",
    "        # = (e^z2sum(e^z2))/(sum(e^z2)^2)-e^2z2/(sum(e^z2)^2))\n",
    "        # = e^z2/sum(e^z2)-(e^z2/sum(e^z2))^2\n",
    "        # = y_hat-y_hat^2 = y_hat (1-y_hat)\n",
    "        dyhatz2 = y_hat * (1 - y_hat)\n",
    "        # Derivative of z2 with respect to w2 = h1\n",
    "        dz2w2 = h1\n",
    "        dw2 = ((djyhat * dyhatz2).T @ dz2w2).T + 2 * w2 ** 2\n",
    "        \n",
    "        # db2 = J/b2 = J/yhat * yhat/z2 * z2/b2\n",
    "        # Derivative of z2 with respect to b2 is [1...]\n",
    "        dz2b2 = np.ones_like(self.b2)\n",
    "        db2 = ((djyhat.T @ dyhatz2) @ dz2b2.T).T\n",
    "        \n",
    "        # dw1 = J/w1 = J/y_hat * y_hat/z2 * z2/h1 * h1/z1 * z1/w1\n",
    "        dz2h1 = w2.T\n",
    "        # Similarly to softmax, sigmoid's derivative can be written in terms of the \n",
    "        # original function\n",
    "        dh1z1 = h1 * (1 - h1)\n",
    "        dz1w1 = x\n",
    "        dw1 = (((dyhatz2 @ dz2h1) * dh1z1).T @ dz1w1).T\n",
    "        \n",
    "        # db1 = J/b1 = J/y_hat * y_hat/z2 * z2/h1 * h1/z1 * z1/b1\n",
    "        # z1/b1 = [1...]\n",
    "        dz1b1 = np.ones_like(self.b1)\n",
    "        db1 = ((dyhatz2 @ dz2h1).T @ dh1z1 @ dz1b1.T).T\n",
    "        \n",
    "        # End\n",
    "        return dw1, db1, dw2, db2\n",
    "\n",
    "    def categorical_cross_entropy_loss(self,\n",
    "                                       y_hat: np.ndarray,\n",
    "                                       y: np.ndarray) -> Union[float, np.ndarray]:\n",
    "        \"\"\" Calculate the binary cross-entropy loss\n",
    "        Args:\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "        Returns:\n",
    "            loss: BCE loss\n",
    "        \"\"\"\n",
    "        y_hat = np.clip(y_hat, a_min=self.eps, a_max=1 - self.eps)\n",
    "        loss = 0\n",
    "        # Start your code here (Calculate the binary cross-entropy)\n",
    "        # Apply BCE with L2 regularization. It seems the weights are added \n",
    "        # for each input, so over all inputs n copies of the weights are added\n",
    "        # then divided by n and multiplied by -1 so overall a single copy is \n",
    "        # subtracted. The sum bounds (K=number of class by d = weight dimension)\n",
    "        # implies w2 rather than w1\n",
    "        loss = -1 / len(y) * np.sum(y * np.log(y_hat)) + self.lambda_ * np.sum(self.w2 ** 2)\n",
    "        # End\n",
    "        return loss\n",
    "\n",
    "    def gradient_descent(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float], lr: float):\n",
    "        self.w1 -= lr * dw1\n",
    "        self.b1 -= lr * db1\n",
    "        self.w2 -= lr * dw2\n",
    "        self.b2 -= lr * db2\n",
    "\n",
    "    def predict(self, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict the label using the output y_hat\n",
    "        Args:\n",
    "            y_hat: Model output\n",
    "        Returns:\n",
    "            pred: Prediction\n",
    "        \"\"\"\n",
    "        pred = np.zeros_like(y_hat)\n",
    "        index = np.argmax(y_hat, axis=-1)\n",
    "        pred[np.arange(len(y_hat)), index] = 1\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation Metrics\n",
    "\n",
    "Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def get_metrics(y_pred: np.ndarray, y_true: np.ndarray) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Calculate the accuracy, precision, recall, and f1 score.\n",
    "        You are allowed to use precision_recall_fscore_support from scikit-learn. Please set average to 'micro'\n",
    "    Args:\n",
    "        y_pred: Prediction\n",
    "        y_true: Ground-truth\n",
    "    Returns:\n",
    "        accuracy: float number. The accuracy for the whole dataset\n",
    "        precision, recall, f1: np.ndarray (num_class, ). The precision, recall, f1 for each class\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y_true.shape\n",
    "    accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "    # Start your code here\n",
    "    # Use sklearn to get precision, recall, fscore for each class\n",
    "    # the docstring above says to set average to micro, but everything else (returns description, \n",
    "    # typehint) indicates we want average=None here... So I just set average=None. \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"micro\", zero_division=0.0)\n",
    "    # Accuracy is correct/total\n",
    "    accuracy = np.sum(y_true == y_pred) / np.size(y_true)\n",
    "    # End\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 AdaGrad (5 points)\n",
    "\n",
    "$$ \\mathbf{G}^{(t + 1)} \\leftarrow \\mathbf{G}^{(t)} + \\boldsymbol{g}^{(t + 1)} \\cdot \\boldsymbol{g}^{(t + 1)} $$\n",
    "$$ \\mathbf{w}^{(t + 1)} \\leftarrow \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}}\\boldsymbol{g}^{(t + 1)} = \\mathbf{w}^{(t)} - \\eta\\frac{\\boldsymbol{g}^{(t + 1)}}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, init_lr, model):\n",
    "        self.init_lr = init_lr\n",
    "        self.model = model\n",
    "        \n",
    "        self.accumulative_dw1 = 0\n",
    "        self.accumulative_db1 = 0\n",
    "        self.accumulative_dw2 = 0\n",
    "        self.accumulative_db2 = 0\n",
    "        self.eps = 1e-9\n",
    "        \n",
    "    def update(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float]):\n",
    "        \"\"\" 1. Use the gradient in the current step to update the accumulative gradient of each parameter.\n",
    "            2. Calculate the new gradient with the accumulative gradient\n",
    "            3. Use the init learning rate the new gradient to update the parameter with model.gradient_descent()\n",
    "        \n",
    "        Do not return anything\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1\n",
    "        self.accumulative_dw1 += dw1 ** 2\n",
    "        self.accumulative_db1 += db1 ** 2\n",
    "        self.accumulative_dw2 += dw2 ** 2\n",
    "        self.accumulative_db2 += db2 ** 2\n",
    "        # Step 2\n",
    "        ndw1 = dw1 / np.sqrt(self.accumulative_dw1 + self.eps)\n",
    "        ndb1 = db1 / np.sqrt(self.accumulative_db1 + self.eps)\n",
    "        ndw2 = dw2 / np.sqrt(self.accumulative_dw2 + self.eps)\n",
    "        ndb2 = db2 / np.sqrt(self.accumulative_db2 + self.eps)\n",
    "        # Step 3\n",
    "        self.model.gradient_descent(ndw1, ndb1, ndw2, ndb2, self.init_lr)\n",
    "        # End\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Mini-batch Gradient Descent (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def train_mbgd(model: 'MLP',\n",
    "               x_train: np.ndarray,\n",
    "               y_train: np.ndarray,\n",
    "               x_valid: np.ndarray,\n",
    "               y_valid: np.ndarray,\n",
    "               lr: float,\n",
    "               num_epoch: int,\n",
    "               batch_size: int,\n",
    "               print_every: int = 10) -> Tuple[dict[str, List], dict[str, List]]:\n",
    "    \"\"\" Training with Gradient Descent\n",
    "    Args:\n",
    "        model: The logistic regression model\n",
    "        x_train: Training feature, (n x d) matrix\n",
    "        y_train: Training label, (n, ) vector\n",
    "        x_valid: Validation feature, (n x d) matrix\n",
    "        y_valid: Validation label, (n, ) vector\n",
    "        lr: Learning rate\n",
    "        num_epoch: Number of training epochs\n",
    "        batch_size: Number of training samples in a batch\n",
    "        print_every: Print log every {print_every} epochs\n",
    "    Returns:\n",
    "        train_history: Log of training information. The format of training history is\n",
    "                       { 'loss': [] }\n",
    "                       It records the average loss of each epoch.\n",
    "        valid_history: Log of validation information. The format of training and validation history is\n",
    "                       {\n",
    "                           'loss': [],\n",
    "                           'accuracy': [],\n",
    "                           'precision': [],\n",
    "                           'recall': [],\n",
    "                           'f1': []\n",
    "                       }\n",
    "    \"\"\"\n",
    "    train_history = OrderedDict({'loss': []})\n",
    "    valid_history = OrderedDict({\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    })\n",
    "\n",
    "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
    "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
    "        train_log = ' - '.join([f'train_{key}: {(val[-1]):.4f}' for key, val in train_history.items()])\n",
    "        valid_log = ' - '.join([f'valid_{key}: {(val[-1]):.4f}' for key, val in valid_history.items()])\n",
    "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
    "        return log\n",
    "\n",
    "    # IMPORTANT: YOU SHOULD USE THIS OPTIMIZER TO UPDATE THE MODEL\n",
    "    optimizer = AdaGrad(init_lr=lr, model=model)\n",
    "\n",
    "    train_num_samples = len(x_train)\n",
    "    n_batch = train_num_samples // batch_size\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        # Start your code here (training)\n",
    "        #     Step 1. Model forward\n",
    "        #     Step 2. Calculate loss\n",
    "        #     Step 3. Model backward\n",
    "        #     Step 4. Optimization with Adagrad\n",
    "        # Get batches using np.array_split\n",
    "        for batch_x, batch_y in zip(np.array_split(x_train, n_batch, axis=0),\n",
    "                                    np.array_split(y_train, n_batch, axis=0)):\n",
    "            # Model forward with return hiddens\n",
    "            y_hat, h1 = model.forward(batch_x, True)\n",
    "            # Get the loss\n",
    "            epoch_loss = model.categorical_cross_entropy_loss(y_hat, batch_y)\n",
    "            # Backpropogation\n",
    "            dw1, db1, dw2, db2 = model.backward(batch_x, y_hat, batch_y, h1)\n",
    "            # Optimize\n",
    "            optimizer.update(dw1, db1, dw2, db2)\n",
    "            \n",
    "        # End\n",
    "\n",
    "        valid_loss = 0.\n",
    "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        # Start your code here (validation)\n",
    "        #     Step 1. Predict\n",
    "        #     Step 2. Calculate loss\n",
    "        #     Step 3. Calculate metrics\n",
    "        y_hat = model.forward(x_valid)\n",
    "        valid_loss = model.categorical_cross_entropy_loss(y_hat, y_valid)\n",
    "        accuracy, precision, recall, f1 = get_metrics(model.predict(y_hat), y_valid)\n",
    "        # End\n",
    "\n",
    "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
    "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
    "            vals.append(val)\n",
    "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
    "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
    "            print(log)\n",
    "        else:\n",
    "            print_line(log)\n",
    "\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100: train_loss: 0.0001 - valid_loss: 1.1941 - valid_accuracy: 0.7162 - valid_precision: 0.5743 - valid_recall: 0.5743 - valid_f1: 0.5743\n",
      "Epoch 11 / 100: train_loss: 0.0001 - valid_loss: 2.0932 - valid_accuracy: 0.7162 - valid_precision: 0.5743 - valid_recall: 0.5743 - valid_f1: 0.5743\n",
      "Epoch 21 / 100: train_loss: 0.0001 - valid_loss: 2.5287 - valid_accuracy: 0.7162 - valid_precision: 0.5743 - valid_recall: 0.5743 - valid_f1: 0.5743\n",
      "Epoch 31 / 100: train_loss: 0.0001 - valid_loss: 2.8906 - valid_accuracy: 0.7157 - valid_precision: 0.5736 - valid_recall: 0.5736 - valid_f1: 0.5736\n",
      "Epoch 41 / 100: train_loss: 0.0001 - valid_loss: 3.2058 - valid_accuracy: 0.7157 - valid_precision: 0.5736 - valid_recall: 0.5736 - valid_f1: 0.5736\n",
      "Epoch 51 / 100: train_loss: 0.0001 - valid_loss: 3.4024 - valid_accuracy: 0.7157 - valid_precision: 0.5736 - valid_recall: 0.5736 - valid_f1: 0.5736\n",
      "Epoch 61 / 100: train_loss: 0.0001 - valid_loss: 3.4507 - valid_accuracy: 0.7157 - valid_precision: 0.5736 - valid_recall: 0.5736 - valid_f1: 0.5736\n",
      "Epoch 71 / 100: train_loss: 0.0001 - valid_loss: 3.4779 - valid_accuracy: 0.7157 - valid_precision: 0.5736 - valid_recall: 0.5736 - valid_f1: 0.5736\n",
      "Epoch 81 / 100: train_loss: 0.0001 - valid_loss: 3.4981 - valid_accuracy: 0.7157 - valid_precision: 0.5736 - valid_recall: 0.5736 - valid_f1: 0.5736\n",
      "Epoch 91 / 100: train_loss: 0.0001 - valid_loss: 3.5129 - valid_accuracy: 0.7157 - valid_precision: 0.5736 - valid_recall: 0.5736 - valid_f1: 0.5736\n",
      "Epoch 100 / 100: train_loss: 0.0001 - valid_loss: 3.5232 - valid_accuracy: 0.7157 - valid_precision: 0.5736 - valid_recall: 0.5736 - valid_f1: 0.5736\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "mbgd_train_history, mbgd_valid_history = train_mbgd(model_mbgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, batch_size, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 MLP using Tensorflow (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "\n",
    "class MLPTF(Model):\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model using tensorflow.keras\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        self.dense1 = Dense(hidden_dim, activation=sigmoid, use_bias=True)\n",
    "        self.dense2 = Dense(num_class, activation=tf.keras.activations.linear, use_bias=True)\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "        # End\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Forward function of tf. It should be named 'call'\n",
    "        \n",
    "        Args:\n",
    "            x: (n x feature_dim) tensor\n",
    "        Returns:\n",
    "            y_hat: (n x num_class) tensor\n",
    "        \"\"\"\n",
    "        # Start your code here (Forward)\n",
    "        y_hat = self.softmax(self.dense2(self.dense1(x)))\n",
    "        # End\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aidan/.pyenv/versions/3.11.8/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlptf\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               multiple                  12800     \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  387       \n",
      "                                                                 \n",
      " softmax (Softmax)           multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13187 (51.51 KB)\n",
      "Trainable params: 13187 (51.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 22:27:04.735639: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 22:27:04.735821: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 22:27:04.735866: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 22:27:04.912238: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 22:27:04.912317: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 22:27:04.912329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-04 22:27:04.912366: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 22:27:04.912402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "np.random.seed(6666)\n",
    "tf.random.set_seed(6666)\n",
    "\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-1\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "\n",
    "model_tf = MLPTF(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "model_tf.build(input_shape=(None, vocab_size))\n",
    "model_tf.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(num_classes=num_class, average='micro')])\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 22:27:07.326646: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-04 22:27:07.567896: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fb8943330b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-04 22:27:07.567952: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6\n",
      "2024-03-04 22:27:07.596979: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-04 22:27:07.668946: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709609227.737561    5476 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 3s 10ms/step - loss: 1.0204 - categorical_accuracy: 0.5286 - f1_score: 0.5286 - val_loss: 0.9444 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9712 - categorical_accuracy: 0.5604 - f1_score: 0.5604 - val_loss: 0.9608 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.9605 - categorical_accuracy: 0.5602 - f1_score: 0.5602 - val_loss: 0.9443 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9623 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9556 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9553 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9500 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9591 - categorical_accuracy: 0.5595 - f1_score: 0.5595 - val_loss: 0.9546 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9575 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9481 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9535 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9478 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9544 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9549 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9525 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9476 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9516 - categorical_accuracy: 0.5654 - f1_score: 0.5654 - val_loss: 0.9478 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9499 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9633 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9503 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9513 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9486 - categorical_accuracy: 0.5655 - f1_score: 0.5655 - val_loss: 0.9607 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9492 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9505 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9487 - categorical_accuracy: 0.5659 - f1_score: 0.5659 - val_loss: 0.9472 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9468 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9512 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9471 - categorical_accuracy: 0.5653 - f1_score: 0.5653 - val_loss: 0.9477 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9454 - categorical_accuracy: 0.5654 - f1_score: 0.5654 - val_loss: 0.9508 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - -1s -13433us/step - loss: 0.9453 - categorical_accuracy: 0.5655 - f1_score: 0.5655 - val_loss: 0.9553 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9435 - categorical_accuracy: 0.5654 - f1_score: 0.5654 - val_loss: 0.9491 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9427 - categorical_accuracy: 0.5655 - f1_score: 0.5655 - val_loss: 0.9663 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9430 - categorical_accuracy: 0.5662 - f1_score: 0.5662 - val_loss: 0.9502 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9403 - categorical_accuracy: 0.5659 - f1_score: 0.5659 - val_loss: 0.9559 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9395 - categorical_accuracy: 0.5662 - f1_score: 0.5662 - val_loss: 0.9507 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9384 - categorical_accuracy: 0.5658 - f1_score: 0.5658 - val_loss: 0.9688 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9374 - categorical_accuracy: 0.5660 - f1_score: 0.5660 - val_loss: 0.9635 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9370 - categorical_accuracy: 0.5669 - f1_score: 0.5669 - val_loss: 0.9603 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9361 - categorical_accuracy: 0.5669 - f1_score: 0.5669 - val_loss: 0.9539 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9341 - categorical_accuracy: 0.5673 - f1_score: 0.5673 - val_loss: 0.9614 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9330 - categorical_accuracy: 0.5679 - f1_score: 0.5679 - val_loss: 0.9548 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.9317 - categorical_accuracy: 0.5682 - f1_score: 0.5682 - val_loss: 0.9552 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.9295 - categorical_accuracy: 0.5681 - f1_score: 0.5681 - val_loss: 0.9579 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9283 - categorical_accuracy: 0.5687 - f1_score: 0.5687 - val_loss: 0.9578 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.9276 - categorical_accuracy: 0.5689 - f1_score: 0.5689 - val_loss: 0.9626 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 0.9256 - categorical_accuracy: 0.5694 - f1_score: 0.5694 - val_loss: 0.9595 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.9247 - categorical_accuracy: 0.5701 - f1_score: 0.5701 - val_loss: 0.9601 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 0.9225 - categorical_accuracy: 0.5708 - f1_score: 0.5708 - val_loss: 0.9620 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 0.9208 - categorical_accuracy: 0.5703 - f1_score: 0.5703 - val_loss: 0.9625 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.9189 - categorical_accuracy: 0.5712 - f1_score: 0.5712 - val_loss: 0.9633 - val_categorical_accuracy: 0.5743 - val_f1_score: 0.5743\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9167 - categorical_accuracy: 0.5725 - f1_score: 0.5725 - val_loss: 0.9649 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9154 - categorical_accuracy: 0.5725 - f1_score: 0.5725 - val_loss: 0.9654 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9139 - categorical_accuracy: 0.5749 - f1_score: 0.5749 - val_loss: 0.9673 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9128 - categorical_accuracy: 0.5752 - f1_score: 0.5752 - val_loss: 0.9711 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.9110 - categorical_accuracy: 0.5753 - f1_score: 0.5753 - val_loss: 0.9743 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.9084 - categorical_accuracy: 0.5769 - f1_score: 0.5769 - val_loss: 0.9745 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9062 - categorical_accuracy: 0.5788 - f1_score: 0.5788 - val_loss: 0.9744 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9027 - categorical_accuracy: 0.5794 - f1_score: 0.5794 - val_loss: 0.9810 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9013 - categorical_accuracy: 0.5811 - f1_score: 0.5811 - val_loss: 0.9800 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9004 - categorical_accuracy: 0.5820 - f1_score: 0.5820 - val_loss: 0.9777 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8975 - categorical_accuracy: 0.5834 - f1_score: 0.5834 - val_loss: 0.9801 - val_categorical_accuracy: 0.5723 - val_f1_score: 0.5723\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - -2s -13924us/step - loss: 0.8956 - categorical_accuracy: 0.5848 - f1_score: 0.5848 - val_loss: 0.9843 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8928 - categorical_accuracy: 0.5866 - f1_score: 0.5866 - val_loss: 0.9880 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8908 - categorical_accuracy: 0.5887 - f1_score: 0.5887 - val_loss: 0.9897 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8877 - categorical_accuracy: 0.5900 - f1_score: 0.5900 - val_loss: 0.9888 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5736\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8855 - categorical_accuracy: 0.5905 - f1_score: 0.5905 - val_loss: 0.9906 - val_categorical_accuracy: 0.5711 - val_f1_score: 0.5711\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8828 - categorical_accuracy: 0.5956 - f1_score: 0.5956 - val_loss: 0.9931 - val_categorical_accuracy: 0.5730 - val_f1_score: 0.5730\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.8804 - categorical_accuracy: 0.5962 - f1_score: 0.5962 - val_loss: 0.9991 - val_categorical_accuracy: 0.5685 - val_f1_score: 0.5685\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8780 - categorical_accuracy: 0.6005 - f1_score: 0.6005 - val_loss: 0.9981 - val_categorical_accuracy: 0.5704 - val_f1_score: 0.5704\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8754 - categorical_accuracy: 0.6005 - f1_score: 0.6005 - val_loss: 1.0000 - val_categorical_accuracy: 0.5704 - val_f1_score: 0.5704\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8725 - categorical_accuracy: 0.6059 - f1_score: 0.6059 - val_loss: 1.0029 - val_categorical_accuracy: 0.5704 - val_f1_score: 0.5704\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8692 - categorical_accuracy: 0.6059 - f1_score: 0.6059 - val_loss: 1.0081 - val_categorical_accuracy: 0.5615 - val_f1_score: 0.5615\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8672 - categorical_accuracy: 0.6102 - f1_score: 0.6102 - val_loss: 1.0085 - val_categorical_accuracy: 0.5685 - val_f1_score: 0.5685\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8639 - categorical_accuracy: 0.6132 - f1_score: 0.6132 - val_loss: 1.0115 - val_categorical_accuracy: 0.5685 - val_f1_score: 0.5685\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8611 - categorical_accuracy: 0.6160 - f1_score: 0.6160 - val_loss: 1.0225 - val_categorical_accuracy: 0.5576 - val_f1_score: 0.5576\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.8579 - categorical_accuracy: 0.6213 - f1_score: 0.6213 - val_loss: 1.0182 - val_categorical_accuracy: 0.5691 - val_f1_score: 0.5691\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.8557 - categorical_accuracy: 0.6224 - f1_score: 0.6224 - val_loss: 1.0214 - val_categorical_accuracy: 0.5685 - val_f1_score: 0.5685\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8522 - categorical_accuracy: 0.6254 - f1_score: 0.6254 - val_loss: 1.0267 - val_categorical_accuracy: 0.5685 - val_f1_score: 0.5685\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.8491 - categorical_accuracy: 0.6280 - f1_score: 0.6280 - val_loss: 1.0299 - val_categorical_accuracy: 0.5666 - val_f1_score: 0.5666\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 0.8455 - categorical_accuracy: 0.6334 - f1_score: 0.6334 - val_loss: 1.0381 - val_categorical_accuracy: 0.5691 - val_f1_score: 0.5691\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8433 - categorical_accuracy: 0.6350 - f1_score: 0.6350 - val_loss: 1.0352 - val_categorical_accuracy: 0.5615 - val_f1_score: 0.5615\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8397 - categorical_accuracy: 0.6407 - f1_score: 0.6407 - val_loss: 1.0409 - val_categorical_accuracy: 0.5615 - val_f1_score: 0.5615\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8374 - categorical_accuracy: 0.6441 - f1_score: 0.6441 - val_loss: 1.0430 - val_categorical_accuracy: 0.5615 - val_f1_score: 0.5615\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8338 - categorical_accuracy: 0.6457 - f1_score: 0.6457 - val_loss: 1.0475 - val_categorical_accuracy: 0.5602 - val_f1_score: 0.5602\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8302 - categorical_accuracy: 0.6496 - f1_score: 0.6496 - val_loss: 1.0528 - val_categorical_accuracy: 0.5595 - val_f1_score: 0.5595\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8274 - categorical_accuracy: 0.6529 - f1_score: 0.6529 - val_loss: 1.0577 - val_categorical_accuracy: 0.5615 - val_f1_score: 0.5615\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8245 - categorical_accuracy: 0.6565 - f1_score: 0.6565 - val_loss: 1.0608 - val_categorical_accuracy: 0.5615 - val_f1_score: 0.5615\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8217 - categorical_accuracy: 0.6594 - f1_score: 0.6594 - val_loss: 1.0646 - val_categorical_accuracy: 0.5595 - val_f1_score: 0.5595\n",
      "Epoch 79/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8182 - categorical_accuracy: 0.6631 - f1_score: 0.6631 - val_loss: 1.0702 - val_categorical_accuracy: 0.5595 - val_f1_score: 0.5595\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8151 - categorical_accuracy: 0.6684 - f1_score: 0.6684 - val_loss: 1.0735 - val_categorical_accuracy: 0.5583 - val_f1_score: 0.5583\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8110 - categorical_accuracy: 0.6699 - f1_score: 0.6699 - val_loss: 1.0789 - val_categorical_accuracy: 0.5595 - val_f1_score: 0.5595\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8084 - categorical_accuracy: 0.6724 - f1_score: 0.6724 - val_loss: 1.0834 - val_categorical_accuracy: 0.5531 - val_f1_score: 0.5531\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.8053 - categorical_accuracy: 0.6736 - f1_score: 0.6736 - val_loss: 1.0897 - val_categorical_accuracy: 0.5512 - val_f1_score: 0.5512\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8026 - categorical_accuracy: 0.6793 - f1_score: 0.6793 - val_loss: 1.0959 - val_categorical_accuracy: 0.5595 - val_f1_score: 0.5595\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7998 - categorical_accuracy: 0.6805 - f1_score: 0.6805 - val_loss: 1.0996 - val_categorical_accuracy: 0.5442 - val_f1_score: 0.5442\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - -1s -12181us/step - loss: 0.7965 - categorical_accuracy: 0.6849 - f1_score: 0.6849 - val_loss: 1.1067 - val_categorical_accuracy: 0.5576 - val_f1_score: 0.5576\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7929 - categorical_accuracy: 0.6859 - f1_score: 0.6859 - val_loss: 1.1093 - val_categorical_accuracy: 0.5499 - val_f1_score: 0.5499\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7894 - categorical_accuracy: 0.6901 - f1_score: 0.6901 - val_loss: 1.1168 - val_categorical_accuracy: 0.5531 - val_f1_score: 0.5531\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7862 - categorical_accuracy: 0.6928 - f1_score: 0.6928 - val_loss: 1.1211 - val_categorical_accuracy: 0.5467 - val_f1_score: 0.5467\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7831 - categorical_accuracy: 0.6940 - f1_score: 0.6940 - val_loss: 1.1286 - val_categorical_accuracy: 0.5269 - val_f1_score: 0.5269\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7802 - categorical_accuracy: 0.6965 - f1_score: 0.6965 - val_loss: 1.1334 - val_categorical_accuracy: 0.5423 - val_f1_score: 0.5423\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7768 - categorical_accuracy: 0.6982 - f1_score: 0.6982 - val_loss: 1.1385 - val_categorical_accuracy: 0.5301 - val_f1_score: 0.5301\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7737 - categorical_accuracy: 0.7010 - f1_score: 0.7010 - val_loss: 1.1444 - val_categorical_accuracy: 0.5467 - val_f1_score: 0.5467\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7705 - categorical_accuracy: 0.7021 - f1_score: 0.7021 - val_loss: 1.1499 - val_categorical_accuracy: 0.5301 - val_f1_score: 0.5301\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7678 - categorical_accuracy: 0.7043 - f1_score: 0.7043 - val_loss: 1.1580 - val_categorical_accuracy: 0.5506 - val_f1_score: 0.5506\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7644 - categorical_accuracy: 0.7073 - f1_score: 0.7073 - val_loss: 1.1621 - val_categorical_accuracy: 0.5371 - val_f1_score: 0.5371\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7614 - categorical_accuracy: 0.7083 - f1_score: 0.7083 - val_loss: 1.1691 - val_categorical_accuracy: 0.5237 - val_f1_score: 0.5237\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7585 - categorical_accuracy: 0.7102 - f1_score: 0.7102 - val_loss: 1.1761 - val_categorical_accuracy: 0.5179 - val_f1_score: 0.5179\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7558 - categorical_accuracy: 0.7121 - f1_score: 0.7121 - val_loss: 1.1835 - val_categorical_accuracy: 0.5147 - val_f1_score: 0.5147\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7527 - categorical_accuracy: 0.7158 - f1_score: 0.7158 - val_loss: 1.1891 - val_categorical_accuracy: 0.5147 - val_f1_score: 0.5147\n"
     ]
    }
   ],
   "source": [
    "tf_history = model_tf.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=batch_size, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation with Tensroflow\n",
    "You are required to report the loss, accuracy, precision, recall, and f1 on test set and plot the the curve of them for both SGD and Mini-batch GD on train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch GD: (0.7087279242385462, 0.5630918863578193, 0.5630918863578193, 0.5630918863578193)\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.9404 - categorical_accuracy: 0.5631 - f1_score: 0.5631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9403693079948425, 0.5630918741226196, 0.5630918741226196]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the metrics for test set and fill in the table below\n",
    "y_hat = model_mbgd.forward(x_test)\n",
    "y_pred = model_mbgd.predict(y_hat)\n",
    "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
    "model_tf.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics on Test set\n",
    "Fill this table with the result you just printed (double click this cell to edit)\n",
    "|     Optimizer                     | Accuracy    | F1 Score    |\n",
    "|:---------------------------------:|-------------|-------------|\n",
    "|      **Your Implementation**      |       0.5631|       0.5631|\n",
    "| **Tensorflow**                    |       0.5631|       0.5631|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the training loss curve for Your implementation and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.plot(mbgd_train_history['loss'], label='My MLP')\n",
    "plt.plot(tf_history.history['loss'], label='TF MLP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the validation metrics curve for SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax = axes[0]\n",
    "ax.plot(mbgd_valid_history['accuracy'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_categorical_accuracy'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation Accuracy')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(mbgd_valid_history['f1'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_f1_score'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation F1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion (5 Points)\n",
    "\n",
    "Provide an analysis for all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Both my manually programmed model and tensorflow's model result in similar validation metrics, though neither improved over time and the validation metrics weren't very impressive. \n",
    "\n",
    "Normally, I would suspect I did something wrong with my derivatives, but since tensorflow also didn't perform very well, I'm not entirely sure that I *did* mess up. \n",
    "\n",
    "As for the graphs, my training error looks to be really small, but I think that's a matter of scaling.\n",
    "\n",
    "Validation error does increase over time for my model, but I'm not sure why. In addition, both validation accuracy and f1 *decrease* over time for the tensorflow model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd436f7",
   "metadata": {},
   "source": [
    "Breakdown of the regex used to remove URLs in preprocessing code\n",
    "\n",
    "    (?:https?://)?\\w+\\.\\w+(\\.\\w+?)?(?(1)|(?=/))(?:/\\S*?)*(?:[\\s]|$|[^\\s\\w\\\\\\/\\&\\?\\%\\#])\n",
    "\n",
    "    (?:https?://)? - Matches http or https, if present, but not required to match a link.\n",
    "\n",
    "    (i.e. https://www... or http://www... or just www...)\n",
    "    \\w+\\.\\w+(\\.\\w+?)? - Matches 3 group or 2 group link methods (i.e. www.abc.xyz or just abc.xyz)\n",
    "\n",
    "    (?(1)|(?=/)) - Ensures that if the previous part matches a 2 group, that the link is followed by a forward slash (/). This is to\n",
    "    prevent matching a link on sentences not followed by spaces (i.e preventing blah blah blah.blah blah) from matching blah.blah\n",
    "\n",
    "    (?:/\\S*?)* - matches any number of /... segments, allows link to end in forward slash. (i.e. www.abc.xyz/abc/)\n",
    "\n",
    "    (?:[\\s]|$|[^\\s\\w\\\\\\/\\&\\?\\%\\#]) - Terminates the match if a whitespace character, string end, or a non-whitespace charater that isn't \n",
    "    generally part of urls is encountered, and forces previous lazy matching\n",
    "    \n",
    "    (+? and *?, which match as little as possible) to consume the entire link, but not beyond the link."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
