{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aidan Fischer\n",
    "# CS584 HW1: Text Classification\n",
    "# I pledge my honor that I have abided by the Stevens Honor System\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ham/spam dataset\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(\"a1-data/SMSSpamCollection\", \"r\") as datafile:\n",
    "    for sentence in datafile.readlines():\n",
    "        # Get the label and sentence. Store that in the dataset\n",
    "        split = sentence.split(\"\\t\")\n",
    "        label = split[0]\n",
    "        sentence = \" \".join(split[1:]) \n",
    "        data.append([label, sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation, urls, numbers. Apply lowercase.\n",
    "\n",
    "\n",
    "# Preprocess the data. Start with detecting any urls. If a url, \n",
    "# remove from the sentence. Otherwise apply lowercase and remove\n",
    "# punctuation and number.\n",
    "for i in range(len(data)):\n",
    "    sentence = data[i][1]\n",
    "    words = sentence.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        parsed = urlparse(word)\n",
    "        if parsed.scheme and parsed.netloc:\n",
    "            continue\n",
    "        else:\n",
    "            new_words.append(re.sub(r\"[^a-z]\", \"\", word))\n",
    "    data[i][1] = \" \".join(new_words)\n",
    "    \n",
    "# Randomize order of data\n",
    "rand = np.random.default_rng(seed=181853)\n",
    "data = list(rand.permutation(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization step. Use NLTK\n",
    "# Get stopwords\n",
    "try:\n",
    "    _ = stopwords.words('english')\n",
    "except LookupError: \n",
    "    nltk.download(\"stopwords\")\n",
    "finally:\n",
    "    sw = stopwords.words('english')\n",
    "    \n",
    "# Create the stemmer\n",
    "ps = PorterStemmer()    \n",
    "\n",
    "# Tokenize each sentence, apply stemming, remove stopwords\n",
    "for i in range(len(data)):\n",
    "    data[i] = list(data[i])\n",
    "    sentence = data[i][1]\n",
    "    # Tokenize\n",
    "    try:\n",
    "        _ = word_tokenize(sentence)\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "    finally:\n",
    "        tokenized = word_tokenize(sentence)\n",
    "    # Stem + Stopwords\n",
    "    stemmed = []\n",
    "    for w in tokenized:\n",
    "        s = ps.stem(w)\n",
    "        if s not in sw:\n",
    "            stemmed.append(s)\n",
    "    data[i][1] = stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set           Ham    Spam    Total\n",
      "----------  -----  ------  -------\n",
      "Training     3394     505     3899\n",
      "Validation    956     158     1114\n",
      "Testing       477      84      561\n",
      "Total        4827     747     5574\n"
     ]
    }
   ],
   "source": [
    "# Split into train, validation, test\n",
    "# with \n",
    "# 70% Train\n",
    "# 20% validation\n",
    "# 10% test\n",
    "# Create distribution table as well\n",
    "split = len(data) // 10\n",
    "train = data[:split * 7]\n",
    "validation = data[split * 7: split * 9]\n",
    "test = data[split * 9:]\n",
    "\n",
    "distrib_train = 0\n",
    "distrib_valid = 0\n",
    "distrib_test = 0\n",
    "\n",
    "for elem in train:\n",
    "    if elem[0] == 'ham':\n",
    "        distrib_train += 1\n",
    "for elem in validation:\n",
    "    if elem[0] == 'ham':\n",
    "        distrib_valid += 1\n",
    "for elem in test:\n",
    "    if elem[0] == 'ham':\n",
    "        distrib_test += 1\n",
    "\n",
    "distrib_total = distrib_train + distrib_valid + distrib_test\n",
    "\n",
    "distribution = [\n",
    "    [\"Training\",distrib_train, len(train) - distrib_train, len(train)],\n",
    "    [\"Validation\",distrib_valid, len(validation) - distrib_valid, len(validation)],\n",
    "    [\"Testing\",distrib_test, len(test) - distrib_test, len(test)],\n",
    "    [\"Total\", distrib_total, len(data) - distrib_total, len(data)]\n",
    "]\n",
    "\n",
    "print(tabulate(distribution, headers = [\"Set\", \"Ham\", \"Spam\", \"Total\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c728c1f6a6c9416fbbe75c1b2ed8b364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building doc freq data:   0%|          | 0/5875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TF-IDF Feature extractor\n",
    "# Also convert ham/spam class to 0/1\n",
    "\n",
    "# Build the vocab for TF-IDF\n",
    "vocab = set()\n",
    "for _, sentence in train:\n",
    "    vocab.update(sentence)\n",
    "    \n",
    "# Convert the set back to a list (for strict ordering)\n",
    "vocab = list(vocab)\n",
    "# Not strictly necessary, but I'm doing it anyway\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Build document frequency vector\n",
    "\n",
    "df = [0 for _ in vocab]\n",
    "\n",
    "for i, word in tqdm(enumerate(vocab), \n",
    "                    desc=\"Building doc freq data\",\n",
    "                    total=len(vocab)):\n",
    "    for _, doc in train:\n",
    "        df[i] += doc.count(word)\n",
    "\n",
    "class TF_IDF:\n",
    "    # Initialize with vocab, doc frequency\n",
    "    def __init__(self, vocab, df):\n",
    "        self.vocab = vocab\n",
    "        self.df = df\n",
    "    \n",
    "    # Converter\n",
    "    # [tf/idf,...]\n",
    "    def doc_to_vec(self, doc):\n",
    "        return np.array([doc.count(word)/df[i] for i, word in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "class LogisticRegression:\n",
    "    # Initialization\n",
    "    # x is np array with x_is as columns\n",
    "    # y is column of y_is \n",
    "    # Randomly initialize w\n",
    "    # l is the lambda hyperparameter\n",
    "    # b is the bias vector\n",
    "    # iter is the current training iteration\n",
    "    def __init__(self, train_x, train_y, test_x, test_y, valid_x, valid_y, l, descent_method):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.valid_x = valid_x\n",
    "        self.valid_y = valid_y\n",
    "        self.test_x = test_x\n",
    "        self.test_y = test_y\n",
    "        self.rand = np.random.default_rng(seed=3415)\n",
    "        self.w = self.rand.normal(size=(1,self.train_x.shape[0]))\n",
    "        self.l = l\n",
    "        self.b = self.rand.normal()\n",
    "        self.iter = 0\n",
    "        self.descent_method = descent_method\n",
    "        self.w_grad_sqsum = np.zeros(shape=(1, self.w.shape[1]))\n",
    "        self.b_grad_sqsum = 0\n",
    "    \n",
    "    # Get prediction using sigmoid function\n",
    "    # Use matrix operations\n",
    "    def predict(self, x):\n",
    "        return 1/(1 + np.exp(-(self.w @ x + self.b)))\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    def metrics(self, x, y):\n",
    "        yhat = np.round(self.predict(x))\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for yi, yhati in zip(y.flatten(), yhat.flatten()):\n",
    "            if yi == 1 and int(yhati) == 1:\n",
    "                tp += 1\n",
    "            elif yi == 1 and int(yhati) == 0:\n",
    "                fn += 1\n",
    "            elif yi == 0 and int(yhati) == 1:\n",
    "                fp += 1\n",
    "            elif yi == 0 and int(yhati) == 0:\n",
    "                tn += 1\n",
    "        try:\n",
    "            acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "        except:\n",
    "            acc = 0\n",
    "        try:\n",
    "            prec = tp / (tp + fp)\n",
    "        except:\n",
    "            prec = 0\n",
    "        try:\n",
    "            rec = tp / (tp + fn)\n",
    "        except:\n",
    "            rec = 0\n",
    "        try:\n",
    "            f1 = (prec * rec) / (prec + rec)\n",
    "        except:\n",
    "            f1 = 0\n",
    "        return {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1 Score\": f1}\n",
    "                \n",
    "\n",
    "    # Calculate objective function using matrix operations\n",
    "    def objective(self, x, y):\n",
    "        pred = self.predict(x)\n",
    "        # Avoid errors due to passing 0 to log\n",
    "        pred = np.clip(pred, 1e-8,1-1e-8)\n",
    "        logy = np.log(pred)\n",
    "        log1my = np.log(1 - pred)\n",
    "        bceloss = -1 / x.shape[1] * np.sum(y * logy + (1 - y) * log1my)\n",
    "        reg = self.l * np.sum(self.w ** 2)\n",
    "        return bceloss + reg\n",
    "    \n",
    "    # Calculate objective gradient with respect to w and b\n",
    "    # for specific sample(s) x, y\n",
    "    def obj_gradient(self, x, y):\n",
    "        tmp =  np.exp((-(self.w @ x + self.b)))\n",
    "        denom = 1 + tmp\n",
    "        w_num = x * (y * tmp - (1 - y))\n",
    "        s = np.sum(w_num / denom, axis=1, keepdims=True)\n",
    "        grad_w = -1 / x.shape[1] * s.T + 2 * self.l * self.w\n",
    "        grad_b = -1 / x.shape[1] * np.sum(y - 1 / denom)\n",
    "        return grad_w, grad_b\n",
    "    \n",
    "    # Adagrad functions\n",
    "    def w_adagrad(self):\n",
    "        s = self.w_grad_sqsum / (1 + self.iter)\n",
    "        s += 1e-8\n",
    "        return np.sqrt(s)\n",
    "\n",
    "    def b_adagrad(self):\n",
    "        s = self.b_grad_sqsum / (1 + self.iter)\n",
    "        s += 1e-8\n",
    "        return np.sqrt(s)\n",
    "    \n",
    "    # Gradient descent method. Supports other types (stochastic, batch)\n",
    "    # based on input x, y.\n",
    "    def apply_grad_descent(self, x, y):\n",
    "        grad_w, grad_b = self.obj_gradient(x, y)\n",
    "        self.w_grad_sqsum += grad_w ** 2\n",
    "        self.b_grad_sqsum += grad_b ** 2\n",
    "        self.w -= (.5 / np.sqrt(self.iter + 1)) / (self.w_adagrad() if self.descent_method == \"vanilla\" else 1) * grad_w\n",
    "        self.b -= (.5 / np.sqrt(self.iter + 1)) / (self.b_adagrad() if self.descent_method == \"vanilla\" else 1) * grad_b\n",
    "        \n",
    "    # grad_descent performs gradient descent based on the set descent method\n",
    "    # (vanilla, stochastic, minibatch)\n",
    "    def grad_descent(self):\n",
    "        if self.descent_method == \"vanilla\":\n",
    "            self.apply_grad_descent(self.train_x, self.train_y)\n",
    "        elif self.descent_method == \"stochastic\":\n",
    "            # Permutate the data. the .Ts are there because permutation \n",
    "            # randomizes the order along the first axis (rows), but the columns of x\n",
    "            # are the data.\n",
    "            data_perm = self.rand.permutation(range(self.train_x.shape[1]))\n",
    "            for i in data_perm:\n",
    "                self.apply_grad_descent(np.reshape(self.train_x[:, i], (self.train_x.shape[0], 1)),\n",
    "                                        np.reshape(self.train_y[0, i], (1, 1)))\n",
    "            pass\n",
    "        elif self.descent_method == \"minibatch\":\n",
    "            subset = self.rand.choice(range(self.train_x.shape[1]), size=100)\n",
    "            self.apply_grad_descent(np.reshape(self.train_x[:, subset], (self.train_x.shape[0], 100)),\n",
    "                                    np.reshape(self.train_y[0, subset], (1,100)))\n",
    "        else:\n",
    "            raise ValueError(\"Undefined gradient descent method. \\n\"\n",
    "                             \"Expected one of: ('vanilla', 'stochastic', 'minibatch')\")\n",
    "         \n",
    "    def train(self, max_iter, threshold):\n",
    "        cur_obj = self.objective(self.valid_x, self.valid_y)\n",
    "        stop = False\n",
    "        while self.iter <= max_iter and not stop:\n",
    "            self.grad_descent()\n",
    "            new_obj = self.objective(self.valid_x, self.valid_y)\n",
    "            if np.abs(new_obj - cur_obj) <= threshold:\n",
    "                print(\"\\n\\n\")\n",
    "                print(\"Stopping early due to reaching improvement threshold.\")\n",
    "                stop = True\n",
    "            cur_obj = new_obj\n",
    "            # Delete last iteration output, and print current progress every 100 iterations\n",
    "            if self.iter % 100 == 0:\n",
    "                print(f\"Iteration {self.iter}: Train Loss={self.objective(self.train_x, self.train_y)}, Validation Loss={new_obj}\", end=\"\\r\")\n",
    "            self.iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train, validation, test arrays to feature matrices\n",
    "# Apply standardization\n",
    "tfidf = TF_IDF(vocab, df)\n",
    "\n",
    "def convert_to_feature(data_array):\n",
    "    x = np.array([tfidf.doc_to_vec(doc) for _, doc in data_array]).T\n",
    "    means = np.mean(x, axis = 1, keepdims=True)\n",
    "    stdev = np.std(x, axis = 1, keepdims=True)\n",
    "    stdev[stdev == 0] = 1\n",
    "    y = np.array([[0 if y == \"ham\" else 1 for y, _ in data_array]])\n",
    "    return (x - means) / (stdev), y\n",
    "\n",
    "train_x, train_y = convert_to_feature(train)\n",
    "valid_x, valid_y = convert_to_feature(validation)\n",
    "test_x, test_y = convert_to_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla gradient descent\n",
      "Iteration 5000: Train Loss=0.03527454029897581, Validation Loss=0.121431840169908661\n",
      "Vanilla gradient descent time = 0:08:11.465374 seconds.\n",
      "Minibatch gradient descent\n",
      "Iteration 5000: Train Loss=2.7575663278694162, Validation Loss=3.1517121746192833\n",
      "Minibatch gradient descent time = 0:00:30.108415 seconds.\n",
      "\n",
      "Iteration 50000: Train Loss=0.054970353562601985, Validation Loss=0.22184165739207397\r"
     ]
    }
   ],
   "source": [
    "# Run training for each type of descent\n",
    "print(\"Vanilla gradient descent\")\n",
    "van_start = datetime.now()\n",
    "LR_van = LogisticRegression(train_x, train_y, test_x, test_y, valid_x, valid_y, .01, \"vanilla\")\n",
    "LR_van.train(5000, 1e-15)\n",
    "van_end = datetime.now()\n",
    "print(\"\")\n",
    "print(f\"Vanilla gradient descent time = {van_end - van_start} seconds.\")\n",
    "# Because of how I implement vanilla gradient descent (with matrix operations),\n",
    "# stochastic is far slower than normal (too slow to actually run)\n",
    "# LR_stoch = LogisticRegression(train_x, train_y, test_x, test_y, valid_x, valid_y, .25, \"stochastic\")\n",
    "# print(\"\\n\\n\")\n",
    "# LR_stoch.train(5000, 1e-15)\n",
    "print(\"Minibatch gradient descent\")\n",
    "mini_start = datetime.now()\n",
    "LR_mini = LogisticRegression(train_x, train_y, test_x, test_y, valid_x, valid_y, .01, \"minibatch\")\n",
    "LR_mini.train(5000, 1e-15)\n",
    "mini_end = datetime.now()\n",
    "print(\"\")\n",
    "print(f\"Minibatch gradient descent time = {mini_end - mini_start} seconds.\")\n",
    "\n",
    "\n",
    "LR_mini2 = LogisticRegression(train_x, train_y, test_x, test_y, valid_x, valid_y, .01, \"minibatch\")\n",
    "print(\"\")\n",
    "LR_mini2.train(50000, 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla gradient descent:\n",
      "\n",
      "Metrics      Training    Validation    Testing\n",
      "---------  ----------  ------------  ---------\n",
      "Accuracy     0.997948      0.965889   0.953654\n",
      "Precision    1             1          1\n",
      "Recall       0.984158      0.759494   0.690476\n",
      "F1 Score     0.496008      0.431655   0.408451\n",
      "\n",
      "Minibatch gradient descent:\n",
      "\n",
      "Metrics      Training    Validation    Testing\n",
      "---------  ----------  ------------  ---------\n",
      "Accuracy     0.996409      0.921005   0.942959\n",
      "Precision    0.991984      0.682292   0.802326\n",
      "Recall       0.980198      0.829114   0.821429\n",
      "F1 Score     0.493028      0.374286   0.405882\n",
      "Longer minibatch gradient descent:\n",
      "\n",
      "Metrics      Training    Validation    Testing\n",
      "---------  ----------  ------------  ---------\n",
      "Accuracy     0.996409      0.921005   0.942959\n",
      "Precision    0.991984      0.682292   0.802326\n",
      "Recall       0.980198      0.829114   0.821429\n",
      "F1 Score     0.493028      0.374286   0.405882\n"
     ]
    }
   ],
   "source": [
    "# Print metrics on train, valid, test\n",
    "def print_metrics(LR):\n",
    "    train_met = LR.metrics(LR.train_x, LR.train_y)\n",
    "    valid_met = LR.metrics(LR.valid_x, LR.valid_y)\n",
    "    test_met = LR.metrics(LR.test_x, LR.test_y)\n",
    "\n",
    "    metrics = [\n",
    "        [\"Accuracy\", train_met[\"Accuracy\"], valid_met[\"Accuracy\"], test_met[\"Accuracy\"]],\n",
    "        [\"Precision\", train_met[\"Precision\"], valid_met[\"Precision\"], test_met[\"Precision\"]],\n",
    "        [\"Recall\", train_met[\"Recall\"], valid_met[\"Recall\"], test_met[\"Recall\"]],\n",
    "        [\"F1 Score\", train_met[\"F1 Score\"], valid_met[\"F1 Score\"], test_met[\"F1 Score\"]]\n",
    "    ]\n",
    "\n",
    "    print(tabulate(metrics, headers = [\"Metrics\", \"Training\", \"Validation\", \"Testing\"]))\n",
    "\n",
    "print(\"Vanilla gradient descent:\\n\")\n",
    "print_metrics(LR_van)\n",
    "print(\"\")\n",
    "print(\"Minibatch gradient descent:\\n\")\n",
    "print_metrics(LR_mini)\n",
    "print(\"Longer minibatch gradient descent:\\n\")\n",
    "print_metrics(LR_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, I wasn't have great results. The model kept spitting out around ~0.12 for all inputs,\n",
    "and I wasn't quite sure why, but then I noticed a couple of errors I made (such as accidentally\n",
    "multiplying instead of dividing the root mean of the gradients). That didn't fix it, though it did\n",
    "make the algorithm converge faster.\n",
    "\n",
    "Then I realized that I hadn't normalized the inputs. Once I did that, things went a lot better. \n",
    "The algorithm actually started properly predicting ham/spam.\n",
    "\n",
    "Besides that, regarding vanilla versus stochastic vs minibatch gradient descent, here's my conclusions\n",
    "\n",
    "Vanilla: Slow iterations, but converges to a respectable level in only a few hundred\n",
    "\n",
    "Stochastic: Due to how I implemented vanilla with matrix operations, and how stochastic is a loop by design,\n",
    "stochastic ended up being way too slow per loop over the data to be run in a feasible amount of time.\n",
    "\n",
    "Minibatch: Much faster iterations (about 5000 in the time it takes vanilla to run 300), but converges slower \n",
    "than vanilla (in strictly iteration terms). However, due to the vastly faster iteration time, the algorithm \n",
    "converges to a similar level as vanilla in shorter clock time.\n",
    "\n",
    "In only a few minutes of training, the metrics for this model on the training data are nearly 100%. \n",
    "However, validation and train metrics are lower. Validation error did continuously decrease over time, \n",
    "but this is either a symptom of overfitting or the training data not being a complete representation of\n",
    "the overall data. \n",
    "\n",
    "I did notice, when regularizing the data, that the standard deviation would be zero in some cases (\n",
    "meaning that some words were not used in any of the training documents), so that might contribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2500: Train Loss=0.2356056251897947, Validation Loss=0.314969996100778656\r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del LR_van\n",
    "    del LR_mini\n",
    "    del LR_mini2\n",
    "except:\n",
    "    pass\n",
    "# Set up k-fold cross validation to find best lambda (with k = 10)\n",
    "# Maintain 10% of the data as testing data\n",
    "# Use other 90% in k-fold\n",
    "\n",
    "# Use minibatch\n",
    "split = len(data) // 11\n",
    "\n",
    "test = data[:split]\n",
    "sections = []\n",
    "for i in range(1,11):\n",
    "    sections.append((split*i, split*(i+1)))\n",
    "\n",
    "test_x, test_y = convert_to_feature(test)\n",
    "models = {}\n",
    "lambdas = [0.01,0.02,0.05,0.1,0.2,0.5]\n",
    "\n",
    "for k in range(10):\n",
    "    first, second = sections[k]\n",
    "    validation = data[first:second]\n",
    "    train = data[split:first]\n",
    "    train2 = data[second:]\n",
    "    train.extend(train2)\n",
    "    train_x, train_y = convert_to_feature(train)\n",
    "    valid_x, valid_y = convert_to_feature(validation)\n",
    "    for l in lambdas:\n",
    "        LR = LogisticRegression(train_x, train_y, test_x, test_y, valid_x, valid_y, l, \"minibatch\")\n",
    "        LR.train(2500, 1e-15)\n",
    "        models[(k, l)] = LR.objective(LR.train_x, LR.train_y)\n",
    "        del LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda = 0.05, with an average validation loss of 0.1015926515861364 after 2500 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Which is the best l?\n",
    "min_avg = None\n",
    "\n",
    "for l in lambdas:\n",
    "    acc = 0\n",
    "    for k in range(10):\n",
    "        acc += models[(k, l)]\n",
    "    acc /= 10\n",
    "    if (min_avg is None) or acc < min_avg[1]:\n",
    "        min_avg = (l, acc)\n",
    "print(f\"Best lambda = {min_avg[0]}, with an average validation loss of {min_avg[1]} after 2500 iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set           Jane Austen    Arthur Conan Doyle    Fyodor Dostoyevsky    Total\n",
      "----------  -------------  --------------------  --------------------  -------\n",
      "Training             7757                  1759                  4155    13671\n",
      "Validation           2193                   525                  1188     3906\n",
      "Testing              1104                   254                   601     1959\n",
      "Total               11054                  2538                  5944    19536\n"
     ]
    }
   ],
   "source": [
    "# Load book dataset and perform all preprocessing\n",
    "# then print the data distribution table\n",
    "\n",
    "bookdata = []\n",
    "\n",
    "with open(\"a1-data/books.txt\", \"r\", encoding=\"utf8\") as datafile:\n",
    "    for line in datafile.readlines():\n",
    "        # Get the label and sentence. Store that in the dataset\n",
    "        split = line.split(\"\\t\")\n",
    "        author = split[0]\n",
    "        line = \" \".join(split[1:]) \n",
    "        bookdata.append([author, line])\n",
    "        \n",
    "# Remove punctuation, urls, numbers. Apply lowercase.\n",
    "\n",
    "# Preprocess the data. Start with detecting any urls. If a url, \n",
    "# remove from the sentence. Otherwise apply lowercase and remove\n",
    "# punctuation and number.\n",
    "for i in range(len(bookdata)):\n",
    "    line = bookdata[i][1]\n",
    "    words = line.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        parsed = urlparse(word)\n",
    "        if parsed.scheme and parsed.netloc:\n",
    "            continue\n",
    "        else:\n",
    "            new_words.append(re.sub(r\"[^a-z]\", \"\", word))\n",
    "    bookdata[i][1] = \" \".join(new_words)\n",
    "    \n",
    "# Randomize order of data\n",
    "rand = np.random.default_rng(seed = 31135)\n",
    "bookdata = list(rand.permutation(bookdata))\n",
    "\n",
    "# Tokenization step. Use NLTK\n",
    "# Get stopwords\n",
    "try:\n",
    "    _ = stopwords.words('english')\n",
    "except LookupError: \n",
    "    nltk.download(\"stopwords\")\n",
    "finally:\n",
    "    sw = stopwords.words('english')\n",
    "    \n",
    "# Create the stemmer\n",
    "ps = PorterStemmer()    \n",
    "\n",
    "# Tokenize each sentence, apply stemming, remove stopwords\n",
    "for i in range(len(bookdata)):\n",
    "    bookdata[i] = list(bookdata[i])\n",
    "    line = bookdata[i][1]\n",
    "    # Tokenize\n",
    "    try:\n",
    "        _ = word_tokenize(line)\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "    finally:\n",
    "        tokenized = word_tokenize(line)\n",
    "    # Stem + Stopwords\n",
    "    stemmed = []\n",
    "    for w in tokenized:\n",
    "        s = ps.stem(w)\n",
    "        if s not in sw:\n",
    "            stemmed.append(s)\n",
    "    bookdata[i][1] = stemmed\n",
    "\n",
    "# Split into train, validation, test\n",
    "# with \n",
    "# 70% Train\n",
    "# 20% validation\n",
    "# 10% test\n",
    "# Create distribution table as well\n",
    "split = len(bookdata) // 10\n",
    "train = bookdata[:split * 7]\n",
    "validation = bookdata[split * 7: split * 9]\n",
    "test = bookdata[split * 9:]\n",
    "\n",
    "authors = [\"Jane Austen\", \"Arthur Conan Doyle\", \"Fyodor Dostoyevsky\"]\n",
    "\n",
    "distrib = {dset: {author: 0 for author in authors} for dset in [\"train\", \"valid\", \"test\"]}\n",
    "\n",
    "for elem in train:\n",
    "    distrib[\"train\"][elem[0]] += 1\n",
    "for elem in validation:\n",
    "    distrib[\"valid\"][elem[0]] += 1\n",
    "for elem in test:\n",
    "    distrib[\"test\"][elem[0]] += 1\n",
    "\n",
    "distribution = [\n",
    "    [\"Training\", \n",
    "     distrib[\"train\"][\"Jane Austen\"],\n",
    "     distrib[\"train\"][\"Arthur Conan Doyle\"],\n",
    "     distrib[\"train\"][\"Fyodor Dostoyevsky\"],\n",
    "     distrib[\"train\"][\"Jane Austen\"]+distrib[\"train\"][\"Arthur Conan Doyle\"]+distrib[\"train\"][\"Fyodor Dostoyevsky\"]],\n",
    "    [\"Validation\",distrib[\"valid\"][\"Jane Austen\"],\n",
    "     distrib[\"valid\"][\"Arthur Conan Doyle\"],\n",
    "     distrib[\"valid\"][\"Fyodor Dostoyevsky\"],\n",
    "     distrib[\"valid\"][\"Jane Austen\"]+distrib[\"valid\"][\"Arthur Conan Doyle\"]+distrib[\"valid\"][\"Fyodor Dostoyevsky\"]],\n",
    "    [\"Testing\",distrib[\"test\"][\"Jane Austen\"],\n",
    "     distrib[\"test\"][\"Arthur Conan Doyle\"],\n",
    "     distrib[\"test\"][\"Fyodor Dostoyevsky\"],\n",
    "     distrib[\"test\"][\"Jane Austen\"]+distrib[\"test\"][\"Arthur Conan Doyle\"]+distrib[\"test\"][\"Fyodor Dostoyevsky\"]],\n",
    "    [\"Total\", distrib[\"train\"][\"Jane Austen\"] + distrib[\"valid\"][\"Jane Austen\"] + distrib[\"test\"][\"Jane Austen\"],\n",
    "     distrib[\"train\"][\"Arthur Conan Doyle\"] + distrib[\"valid\"][\"Arthur Conan Doyle\"] + distrib[\"test\"][\"Arthur Conan Doyle\"],\n",
    "     distrib[\"train\"][\"Fyodor Dostoyevsky\"] + distrib[\"valid\"][\"Fyodor Dostoyevsky\"] + distrib[\"test\"][\"Fyodor Dostoyevsky\"],\n",
    "     len(bookdata)]\n",
    "]\n",
    "\n",
    "print(tabulate(distribution, headers = [\"Set\", \"Jane Austen\", \"Arthur Conan Doyle\", \"Fyodor Dostoyevsky\", \"Total\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2002ea4274df4312a32a9fa6b4f3cf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building doc freq data:   0%|          | 0/17015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7adbe198bd449c282ca2effc5f1f5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5d9ea1f4e440a88a970ded529dba0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e2e08d05494d2d87a2d17f9eb8fb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b305782d4d5f42e997a9d76510743ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5539e0561c71407d902990111009bdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1959 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e29ec09220e4aad958d0cdc3441b553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1959 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up TF_IDF for bookdata\n",
    "# Build the vocab for TF-IDF\n",
    "vocab = set()\n",
    "for _, sentence in train:\n",
    "    vocab.update(sentence)\n",
    "    \n",
    "# Convert the set back to a list (for strict ordering)\n",
    "vocab = list(vocab)\n",
    "# Not strictly necessary, but I'm doing it anyway\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Build document frequency vector\n",
    "df = [0 for _ in vocab]\n",
    "\n",
    "for i, word in tqdm(enumerate(vocab), \n",
    "                    desc=\"Building doc freq data\",\n",
    "                    total=len(vocab)):\n",
    "    for _, doc in train:\n",
    "        df[i] += doc.count(word)\n",
    "\n",
    "# Convert bookdata to data matrix\n",
    "tfidf = TF_IDF(vocab, df)\n",
    "\n",
    "def convert_to_feature(data_array):\n",
    "    x = np.array([tfidf.doc_to_vec(doc) for _, doc in tqdm(data_array)]).T\n",
    "    means = np.mean(x, axis = 1, keepdims=True)\n",
    "    stdev = np.std(x, axis = 1, keepdims=True)\n",
    "    stdev[stdev == 0] = 1\n",
    "    y = np.array([[1 if i == authors.index(y) else 0 for i in range(3)] \n",
    "                   for y, _ in tqdm(data_array)]).T\n",
    "    return (x - means) / (stdev), y\n",
    "\n",
    "train_x, train_y = convert_to_feature(train)\n",
    "valid_x, valid_y = convert_to_feature(validation)\n",
    "test_x, test_y = convert_to_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass logistic regression model\n",
    "class MultiClassLogisticRegression:\n",
    "    # Initialization\n",
    "    # x is np array with x_is as columns\n",
    "    # y is column of y_is \n",
    "    # Randomly initialize w\n",
    "    # l is the lambda hyperparameter\n",
    "    # b is the bias vector\n",
    "    # iter is the current training iteration\n",
    "    def __init__(self, train_x, train_y, test_x, test_y, valid_x, valid_y, l, descent_method):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.valid_x = valid_x\n",
    "        self.valid_y = valid_y\n",
    "        self.test_x = test_x\n",
    "        self.test_y = test_y\n",
    "        self.rand = np.random.default_rng(seed=3415)\n",
    "        self.w = self.rand.normal(scale=0.1,size=(train_y.shape[0], self.train_x.shape[0]))\n",
    "        self.l = l\n",
    "        self.iter = 0\n",
    "        self.descent_method = descent_method\n",
    "        self.w_grad_sqsum = np.zeros(shape=(train_y.shape[0], self.w.shape[1]))\n",
    "    \n",
    "    # Get prediction using sigmoid function\n",
    "    # Use matrix operations\n",
    "    def predict(self, x):\n",
    "        tmp = np.exp(self.w @ x)\n",
    "        return tmp / np.sum(tmp, axis=0, keepdims=True)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    def metrics(self, x, y):\n",
    "        yhat = self.predict(x)\n",
    "        table = np.zeros(size=(3,3))\n",
    "        for yi, yhati in zip(y.T, yhat.T):\n",
    "            table[np.argmax(yi), np.argmax(yhati)] += 1\n",
    "        return table\n",
    "\n",
    "    # Calculate objective function using matrix operations\n",
    "    def objective(self, x, y):\n",
    "        pred = self.predict(x)\n",
    "        # Avoid errors due to passing 0 to log\n",
    "        pred = np.clip(pred, 1e-8,1-1e-8)\n",
    "        logp = np.log(pred)\n",
    "        bceloss = -1 / x.shape[1] * np.sum(y * logp)\n",
    "        return bceloss\n",
    "    \n",
    "    # Calculate objective gradient with respect to w and b\n",
    "    # for specific sample(s) x, y\n",
    "    def obj_gradient(self, x, y):\n",
    "        tmp = self.predict(x)\n",
    "        err = y - tmp\n",
    "        grad = np.zeros_like(self.w)\n",
    "        for i in range(x.shape[1]):\n",
    "            grad -= err[:,[i]] @ x[:,[i]].T\n",
    "        return grad\n",
    "    \n",
    "    # Adagrad functions\n",
    "    def w_adagrad(self):\n",
    "        s = self.w_grad_sqsum / (1 + self.iter)\n",
    "        s += 1e-8\n",
    "        return np.sqrt(s)\n",
    "    \n",
    "    # Gradient descent method. Supports other types (stochastic, batch)\n",
    "    # based on input x, y.\n",
    "    def apply_grad_descent(self, x, y):\n",
    "        grad_w = self.obj_gradient(x, y)\n",
    "        self.w_grad_sqsum += grad_w ** 2\n",
    "        self.w -= (.05 / np.sqrt(self.iter + 1)) / (self.w_adagrad() if self.descent_method == \"vanilla\" else 1) * grad_w\n",
    "        \n",
    "    # grad_descent performs gradient descent based on the set descent method\n",
    "    # (vanilla, stochastic, minibatch)\n",
    "    def grad_descent(self):\n",
    "        if self.descent_method == \"vanilla\":\n",
    "            self.apply_grad_descent(self.train_x, self.train_y)\n",
    "        elif self.descent_method == \"stochastic\":\n",
    "            # Permutate the data. the .Ts are there because permutation \n",
    "            # randomizes the order along the first axis (rows), but the columns of x\n",
    "            # are the data.\n",
    "            data_perm = self.rand.permutation(range(self.train_x.shape[1]))\n",
    "            for i in data_perm:\n",
    "                self.apply_grad_descent(np.reshape(self.train_x[:, i], (self.train_x.shape[0], 1)),\n",
    "                                        np.reshape(self.train_y[0, i], (1, 1)))\n",
    "            pass\n",
    "        elif self.descent_method == \"minibatch\":\n",
    "            subset = self.rand.choice(range(self.train_x.shape[1]), size=100)\n",
    "            self.apply_grad_descent(np.reshape(self.train_x[:, subset], (self.train_x.shape[0], 100)),\n",
    "                                    np.reshape(self.train_y[0, subset], (1,100)))\n",
    "        else:\n",
    "            raise ValueError(\"Undefined gradient descent method. \\n\"\n",
    "                             \"Expected one of: ('vanilla', 'stochastic', 'minibatch')\")\n",
    "         \n",
    "    def train(self, max_iter, threshold):\n",
    "        cur_obj = self.objective(self.valid_x, self.valid_y)\n",
    "        stop = False\n",
    "        while self.iter <= max_iter and not stop:\n",
    "            self.grad_descent()\n",
    "            new_obj = self.objective(self.valid_x, self.valid_y)\n",
    "            if np.abs(new_obj - cur_obj) <= threshold:\n",
    "                print(\"\\n\\n\")\n",
    "                print(\"Stopping early due to reaching improvement threshold.\")\n",
    "                stop = True\n",
    "            cur_obj = new_obj\n",
    "            # Delete last iteration output, and print current progress every 100 iterations\n",
    "            if self.iter % 100 == 0:\n",
    "                print(f\"Iteration {self.iter}: Train Loss={self.objective(self.train_x, self.train_y)}, Validation Loss={new_obj}\", end=\"\\r\")\n",
    "            self.iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.5865601057272\n",
      "62.29316181032853\n",
      "528.5462678791644\n",
      "6064.937429127756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aidan\\AppData\\Local\\Temp\\ipykernel_11952\\3116882647.py:28: RuntimeWarning: overflow encountered in exp\n",
      "  tmp = np.exp(self.w @ x)\n",
      "C:\\Users\\Aidan\\AppData\\Local\\Temp\\ipykernel_11952\\3116882647.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  return tmp / np.sum(tmp, axis=0, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334.026126418091in Loss=nan, Validation Loss=7.56059585330592\n",
      "1079.0509616491538\n",
      "243.73724132657986\n",
      "1145.872358495514\n",
      "8988.406554534997\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 100: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 200: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 300: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 400: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 500: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 600: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 700: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 800: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 900: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1000: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1100: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1200: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1300: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1400: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1500: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1600: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1700: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1800: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nanration 1900: Train Loss=nan, Validation Loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "MCLR = MultiClassLogisticRegression(train_x, train_y, test_x, test_y, valid_x, valid_y, 0.05, \"minibatch\")\n",
    "MCLR.train(50000, 1e-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction kept giving overflow errors. Pretty sure I have the right gradient, but it could be wrong. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
